{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced DL and RL: Домашнее задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Студент: Самохвалов Михаил\n",
    "\n",
    "Группа: MADE-DS-32\n",
    "\n",
    "Email: sam2051@yandex.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое ДЗ связано с обучением с подкреплением, и оно придумано для ситуации, когда нейронные сети ещё не нужны, и пространство состояний в целом достаточно маленькое, чтобы можно было обучить хорошую стратегию методами TD-обучения или другими методами обучения с подкреплением. Задание получилось, надеюсь, интересное, но в том числе и достаточно техническое, так что для решения придётся немножко попрограммировать. Поэтому в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Любые комментарии, новые идеи и рассуждения на тему, как всегда, категорически приветствуются.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть первая, с блекджеком и стратегиями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучаться играть в очень простую, но всё-таки знаменитую и популярную игру: блекджек. Правила блекджека достаточно просты; давайте начнём с самой базовой версии, которая реализована в OpenAI Gym:\n",
    " - численные значения карт равны от 2 до 10 для карт от двойки до десятки, 10 для валетов, дам и королей;\n",
    " - туз считается за 11 очков, если общая сумма карт на руке при этом не превосходит 21 (по-английски в этом случае говорят, что на руке есть usable ace), и за 1 очко, если превосходит;\n",
    " - игроку раздаются две карты, дилеру — одна в открытую и одна в закрытую;\n",
    " - игрок может совершать одно из двух действий:\n",
    "    - hit  — взять ещё одну карту;\n",
    "    - stand — не брать больше карт;\n",
    " - если сумма очков у игрока на руках больше 21, он проигрывает (bust);\n",
    " - если игрок выбирает stand с суммой не больше 21, дилер добирает карты, пока сумма карт в его руке меньше 17;\n",
    " - после этого игрок выигрывает, если дилер либо превышает 21, либо получает сумму очков меньше, чем сумма очков у игрока; при равенстве очков объявляется ничья (ставка возвращается);\n",
    " - в исходных правилах есть ещё дополнительный бонус за natural blackjack: если игрок набирает 21 очко с раздачи, двумя картами, он выигрывает не +1, а +1.5 (полторы ставки).\n",
    " \n",
    "Именно этот простейший вариант блекджека реализован в OpenAI Gym:\n",
    "https://github.com/openai/gym/blob/38a1f630dc9815a567aaf299ae5844c8f8b9a6fa/gym/envs/toy_text/blackjack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0', natural=True)\n",
    "observation = env.reset()\n",
    "env.seed(42)\n",
    "# The observation of a 3-tuple of: the players current sum,\n",
    "#     the dealer's one showing card (1-10 where 1 is ace),\n",
    "#     and whether or not the player holds a usable ace (0 or 1) - i.e. use Ace as 11.\n",
    "\n",
    "observation, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_round(strategy):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = strategy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit. Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hit = 1\n",
    "# stand = 0\n",
    "\n",
    "def trivial_strategy(observation):\n",
    "    if observation[0] > 18:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126232ed8050406e8bef9659000d8806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100_000\n",
    "mean_reward = 0\n",
    "\n",
    "for i in tqdm(range(1, n_iter+1)):\n",
    "    reward = play_round(trivial_strategy)\n",
    "    mean_reward += reward / n_iter\n",
    "#     mean_reward += (reward - mean_reward) / i\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда при 100000 сыгранных играх -0.179\n",
      "Средняя награда при 100000 сыгранных играх без natural 0.20\n"
     ]
    }
   ],
   "source": [
    "print(f'Средняя награда при {n_iter} сыгранных играх {mean_reward:.3f}')\n",
    "print(f'Средняя награда при {n_iter} сыгранных играх без natural 0.20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Реализуйте метод обучения с подкреплением без модели (можно Q-обучение, но рекомендую попробовать и другие, например Monte Carlo control) для обучения стратегии в блекджеке, используя окружение Blackjack-v0 из OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning():\n",
    "    \n",
    "    def __init__(self, lr, dr, eps, plot_step=50):\n",
    "        self.lr = lr # learning rate\n",
    "        self.dr = dr # discount rate\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.plt_step = plot_step\n",
    "        \n",
    "        self.env = gym.make('Blackjack-v0', natural=True)\n",
    "#         env.seed(42)\n",
    "        self.Q = defaultdict(lambda: [0.0]*env.action_space.n) # or we can random init here!\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.eps:\n",
    "            action = np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "    \n",
    "    def update_Q(self, state, action, reward):\n",
    "        self.Q[state][action] = self.Q[state][action] + \\\n",
    "                 self.lr * (reward + self.dr * np.max(self.Q[state]) - self.Q[state][action])\n",
    "    \n",
    "    def play_game(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        steps = []\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            steps.append((state, action))\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "        return steps, reward\n",
    "    \n",
    "    def run(self, n_iter, train=True, plot=False):\n",
    "        mean_reward = 0\n",
    "        plot_rewards = []\n",
    "        for i in tqdm(range(n_iter)):\n",
    "            steps, reward = self.play_game()\n",
    "            mean_reward += (reward - mean_reward) / (i + 1)\n",
    "            if i % self.plot\n",
    "            if train:\n",
    "                for state, action in steps[::-1]:\n",
    "                    self.update_Q(state, action, reward)\n",
    "        return mean_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45504ad22ec641c88efa530ea22f620a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befc0f6948bf4baebf94d1aec4b2556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final train mean reward -0.16344999999999982\n",
      "Final test mean reward -0.12524999999999992\n"
     ]
    }
   ],
   "source": [
    "model = QLearning(0.0002, 1.0, 0.2)\n",
    "train_mean_reward = model.run(10000, train=True)\n",
    "test_mean_reward = model.run(10000, train=False)\n",
    "\n",
    "print('Final train mean reward', train_mean_reward)\n",
    "print('Final test mean reward', test_mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = defaultdict(lambda: [0]*env.action_space.n)\n",
    "Q[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
